
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Neural Computation &#8212; Neuromorphic Algorithms Research</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Research/theoretical_neuroscience';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sparse binding demo – What’s the dollar of mexico?" href="sparse_binding_demo.html" />
    <link rel="prev" title="Experimental Neuroscience" href="neuroscience_data.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/neural_circuit.png" class="logo__image only-light" alt="Neuromorphic Algorithms Research - Home"/>
    <script>document.write(`<img src="../_static/neural_circuit.png" class="logo__image only-dark" alt="Neuromorphic Algorithms Research - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Neuromorphic Algorithms Research
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">About me</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../about_me.html">E. Paxon Frady</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Research</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="neuroscience_data.html">Experimental Neuroscience</a></li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">Neural Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="sparse_binding_demo.html">Sparse binding demo – What’s the dollar of mexico?</a></li>
</ul><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="spiking_networks.html">Spiking Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tpam_demo.html">Threshold Phasor Associative Memory</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="resonator_networks.html">Resonator Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="resonator_template.html">Factorization of shape, color and location</a></li>
<li class="toctree-l2"><a class="reference internal" href="res_semi_primes-220216.html">Factoring semi-primes with the resonator network</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Tutorials/computational_neuroscience.html">Computational Neuroscience</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../Tutorials/spiking_basics_brian2.html">Rate-coding with Integrate-and-Fire neurons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Tutorials/connecting_if_neurons.html">Connecting IFR theory to connectionism</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Research/theoretical_neuroscience.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Neural Computation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-theory-of-sequence-indexing-and-working-memory-in-recurrent-neural-networks">A theory of sequence indexing and working memory in recurrent neural networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#links">Links</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-binding-with-sparse-distributed-representations">Variable binding with sparse distributed representations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Links</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-on-functions-using-randomized-vector-representations">Computing on Functions Using Randomized Vector Representations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Links</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#integer-echo-state-networks-efficient-reservoir-computing-for-digital-hardware">Integer Echo State Networks: Efficient Reservoir Computing for Digital Hardware</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Links</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellular-automata-can-reduce-memory-requirements-of-collective-state-computing">Cellular Automata Can Reduce Memory Requirements of Collective-State Computing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Links</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-symbolic-architectures-as-a-computing-framework-for-emerging-hardware">Vector Symbolic Architectures as a Computing Framework for Emerging Hardware</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Links</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="neural-computation">
<h1>Neural Computation<a class="headerlink" href="#neural-computation" title="Link to this heading">#</a></h1>
<p>Our work in neural computation is focused on understanding neural networks and distributed computation starting from fundamental mathematical theories of high-dimensional vector spaces. Using ideas from traditional connectionism, as well as from vector-symbolic architectures (VSA), we developed several new theories and models for performing computations in neural networks in a transparent fashion.</p>
<section id="a-theory-of-sequence-indexing-and-working-memory-in-recurrent-neural-networks">
<h2>A theory of sequence indexing and working memory in recurrent neural networks<a class="headerlink" href="#a-theory-of-sequence-indexing-and-working-memory-in-recurrent-neural-networks" title="Link to this heading">#</a></h2>
<p>E. Paxon Frady, Denis Kleyko, Friedrich T. Sommer (2019). Neural Computation.</p>
<!--
<img src="image_memory_buffer.png" alt="Memory buffer"
	width="60%"/>
-->
<p><img alt="Image memory buffer" src="../_images/image_memory_buffer.png" /></p>
<section id="summary">
<h3>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h3>
<p>Randomized recurrent neural networks, denoted liquid state machines or echo-state networks, were popularized based on their abilities to perform useful computations without the constraint of needing stable states. This inpsired many to think of new ways in which neural networks could be used for computation. However, many treated these networks as a black box, and little was understood as to how they performed their computations.</p>
<p>We identified how such randomized networks could be described based on the principles of Vector Symbolic Architectures. We realized that you could describe the operations of an echo-state network as forming a time-tag, attaching the input to the time-tag, and holding the input history in working memory. This allowed us to derive a precise theory of memory performance and scaling, as well as methods to optimize performance and memory of such networks.</p>
</section>
<section id="links">
<h3>Links<a class="headerlink" href="#links" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://direct.mit.edu/neco/article-abstract/30/6/1449/8400/A-Theory-of-Sequence-Indexing-and-Working-Memory">NeCo</a></p>
<p><a class="reference external" href="https://redwood.berkeley.edu/wp-content/uploads/2020/08/frady-sequence-memory.pdf">PDF</a></p>
</section>
</section>
<section id="variable-binding-with-sparse-distributed-representations">
<h2>Variable binding with sparse distributed representations<a class="headerlink" href="#variable-binding-with-sparse-distributed-representations" title="Link to this heading">#</a></h2>
<p>E. Paxon Frady, Denis Kleyko, Friedrich T. Sommer (2019). IEEE Transactions on Neural Networks and Learning Systems</p>
<!--
<img src="sparse_binding.png" alt="Sparse binding"
	width="75%"/>
-->
<p><img alt="Sparse binding" src="../_images/sparse_binding.png" /></p>
<section id="id1">
<h3>Summary<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>The idea of binding has been part of the connectionist literature for quite some time. The concept of binding is not addressed in modern deep learning much at all, but often studies attempt to perform binding in unnatural ways.</p>
<p>Connectionists in the early 90s realized that there was a need for a way to bind information together in neural representations, and the “superposition catastrophe” was an often cited problem. Early proposals solved this problem by adding the tensor-product operation to connectionism, and Smolensky developed his theory of tensor-product representations.</p>
<p>The tensor product was key to solving the superposition catastrophe because it was a combinatoric operation, and fundamentally this is the issue. Connectionism needed a way to represent “blue triangle” and “red square” in ways that were unique and non-interfering. This fundamentally leads to needing an operation that has a unique result for every combination of shapes and colors.</p>
<p>The combinatorics of tensor products, however, leads to the undersirable result that the neural representation of information is also growing combinatorically. Hinton and other called for proposals where combinatoric representations could be described, but without the growth in dimensionality of the representation. This is where VSA was born, with Plate’s proposals of how to bind information together in a fixed dimensionality that still has access to combinatoric representations.</p>
<p>Since Plate’s proposals, several VSA models were developed which utilize different representations and binding operations. In this paper, we show how all of these VSA models can be unified with tensor-products through the mathematics of compressed sensing. Basically, all VSA binding operations are reductions of the tensor-product.</p>
<p>One of the biggest issues with linking VSA representations to neuroscience is that VSA models are typically dense representation where every vector element would correspond to neural activation. But representations in the brain are highly sparse. This lead us to explore potential new avenues for binding operations, where we desired a binding operation that maintains sparsity.</p>
<p>Based on the link between binding and tensor-product reductions, we explored several methods that could result in a sparsity- and dimensionality-preserving binding operation. We identified a previous proposal based on sparse block-codes that achieved all the desirable features of a sparse VSA, and illustrate how to perform computations with the VSA.</p>
</section>
<section id="id2">
<h3>Links<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://arxiv.org/abs/2009.06734">arXiv</a></p>
<p><a class="reference internal" href="sparse_binding_demo.html"><span class="std std-doc">Demo</span></a> of sparse binding based on Kanerva’s example <a class="reference external" href="https://redwood.berkeley.edu/wp-content/uploads/2020/05/kanerva2010what.pdf">What’s the dollar of Mexico?</a></p>
</section>
</section>
<section id="computing-on-functions-using-randomized-vector-representations">
<h2>Computing on Functions Using Randomized Vector Representations<a class="headerlink" href="#computing-on-functions-using-randomized-vector-representations" title="Link to this heading">#</a></h2>
<p>E. Paxon Frady, Denis Kleyko, Christopher J. Kymn, Bruno A. Olshausen, Friedrich T. Sommer (2021). arXiv.</p>
<p><img alt="vfa_kernels" src="../_images/kernels-210322.png" /></p>
<section id="id3">
<h3>Summary<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>Vector symbolic architectures (VSA) are frameworks for computing with distributed representations like neural networks. There are multiple flavors of the framework, but each is designed to form and manipulate symbols in high-dimensional vector spaces. This is possible, and goes beyond standard artificial neural networks, because of the binding operation in VSA.</p>
<p>In this paper, we extend VSA ideas to vector function architectures (VFA), which can encode and manipulate data on smooth manifolds. This is possible by a fromulation for vector representations of continuous values with fractional power encoding. The idea starts with self-binding of a base vector to create a representation of integers: <span class="math notranslate nohighlight">\(\mathbf{z}^i\)</span> is the vector for representing integer <span class="math notranslate nohighlight">\(i\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{C}^N\)</span> is a random vector. This self-binding process can be extended to continuous values <span class="math notranslate nohighlight">\(x\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{z}^{\beta x}\)</span> represents a continuous number. Such a procedure is a locality-perserving encoding and produces a similarity kernel, where representations of values near <span class="math notranslate nohighlight">\(x\)</span> are similar to the representation for <span class="math notranslate nohighlight">\(x\)</span>, but representations far away are different. The parameter <span class="math notranslate nohighlight">\(\beta\)</span> regulates the width of the similarity kernel.</p>
<p>In the paper, we explain how the VFA can be generalized to other flavors of VSA, such as sparse block-codes. Further, we explain how to create different types of similarity kernels based on the Bochner theorem. Further, we show how multi-dimensional kernels can be formed using the binding operation of 1-D kernels or using jointly sampled base vectors.</p>
<p>Finally, we show a few simple applications that links VFA to other methods in machine-learning, such as kernel methods. We also explain how the VFA architecture can be used to represent images and how image transforms, such as translation, can be implemented through simple VFA operations.</p>
</section>
<section id="id4">
<h3>Links<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://arxiv.org/abs/2109.03429">arXiv</a></p>
<p><a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/3517343.3522597">short version</a></p>
</section>
</section>
<section id="integer-echo-state-networks-efficient-reservoir-computing-for-digital-hardware">
<h2>Integer Echo State Networks: Efficient Reservoir Computing for Digital Hardware<a class="headerlink" href="#integer-echo-state-networks-efficient-reservoir-computing-for-digital-hardware" title="Link to this heading">#</a></h2>
<p>D. Kleyko, E. P. Frady, M. Kheffache and E. Osipov. (2022). IEEE Transactions on Neural Networks and Learning Systems, 33(4): 1688-1701.</p>
<p><img alt="int_esn" src="../_images/int_esn.png" /></p>
<section id="id5">
<h3>Summary<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p>Based on our theoretical analysis of echo-state networks, we were able to use the theory to design an optimized echo-state network for digital hardware implementations. Such a network uses a very simple permutation operation to replace the recurrent weight matrix typically used by reservoir networks, greatly reducing the computational complexity for digital devices. Further, the weights and the neural activations are always constrained to be integers, and we show how to design the integer clipping function to optimally maintain the history of information streaming into the network. We also benchmark the network on a few tasks, and illustrate some coding principles used to use these networks for dynamics modelling and other applications.</p>
</section>
<section id="id6">
<h3>Links<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/9303448">IEEE</a></p>
<p><a class="reference external" href="https://www.researchgate.net/profile/Denis-Kleyko-2/publication/317300050_Integer_Echo_State_Networks_Efficient_Reservoir_Computing_for_Digital_Hardware/links/5f8a8df7458515b7cf8534dc/Integer-Echo-State-Networks-Efficient-Reservoir-Computing-for-Digital-Hardware.pdf">pdf</a></p>
</section>
</section>
<section id="cellular-automata-can-reduce-memory-requirements-of-collective-state-computing">
<h2>Cellular Automata Can Reduce Memory Requirements of Collective-State Computing<a class="headerlink" href="#cellular-automata-can-reduce-memory-requirements-of-collective-state-computing" title="Link to this heading">#</a></h2>
<p>D. Kleyko, E. P. Frady and F. T. Sommer. (2022). IEEE Transactions on Neural Networks and Learning Systems 33(6):2701-2713.</p>
<p><img alt="ca90" src="../_images/ca90.png" /></p>
<section id="id7">
<h3>Summary<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p>Many of the applications which are based on VSA architectures require a large “item memory” to store the atomic vectors used to represent various symbols. These item vectors are typically chosen randomly, but fixed once they are chosen. These vectors, however, can be very large and require lots of memory to store. But by using cellular automata operations, we found a way to store much smaller seed vectors that can be used to reliably generate the large high-dimensional vectors used in the VSA computations. This has the potential to greatly reduce the memory cost of various VSA item memories in hardware, only at the cost of needing to run simple cellular automata for a few iterations.</p>
</section>
<section id="id8">
<h3>Links<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/9586079">IEEE</a></p>
<p><a class="reference external" href="https://redwood.berkeley.edu/wp-content/uploads/2022/04/Cellular_Automata_Can_Reduce_Memory_Requirements_of_Collective-State_Computing.pdf">pdf</a></p>
</section>
</section>
<section id="vector-symbolic-architectures-as-a-computing-framework-for-emerging-hardware">
<h2>Vector Symbolic Architectures as a Computing Framework for Emerging Hardware<a class="headerlink" href="#vector-symbolic-architectures-as-a-computing-framework-for-emerging-hardware" title="Link to this heading">#</a></h2>
<p>Denis Kleyko, Mike Davies, E. Paxon Frady, Pentti Kanerva, Spencer J. Kent, Bruno A. Olshausen, Evgeny Osipov, Jan M. Rabaey, Dmitri A. Rachkovskij, Abbas Rahimi, Friedrich T. Sommer (2022). Proceedings of the IEEE.</p>
<p><img alt="marr_vsa_cover.png" src="../_images/marr_vsa-cover.png" /></p>
<section id="id9">
<h3>Summary<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<p>Our “manifesto” paper lays out the arguments and logic for how VSAs can serve as the computational backbone of emerging computer hardware, like neuromorphic hardware. Much like boolean algebra serves as the mathematical framework by which digital computations are carried, vector symbolic architectures also provide an algebraic mathematical framework for creating parallel/distributed representations akin to neural populations. The mathematical theories behind VSA explain how the frameworks can serve as an intermediate abstraction layer between the implementation and computational levels coined by Marr. One point of highlight is how there are different flavors of VSA, where different flavors use different vector representations and operations, but also retain the same properties and can be expressed by the same algebraic formalism. This paper serves as a survey of many ideas in the VSA field, explain a lot of techniques for building data structures, details many of the recent advances of VSA and how to potentially create applications on neuromorphic hardware following these principles.</p>
</section>
<section id="id10">
<h3>Links<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://ieeexplore.ieee.org/document/9921397">IEEE</a></p>
<p><a class="reference external" href="https://arxiv.org/pdf/2106.05268">pdf (arXiv)</a></p>
</section>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Research"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="neuroscience_data.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Experimental Neuroscience</p>
      </div>
    </a>
    <a class="right-next"
       href="sparse_binding_demo.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Sparse binding demo – What’s the dollar of mexico?</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-theory-of-sequence-indexing-and-working-memory-in-recurrent-neural-networks">A theory of sequence indexing and working memory in recurrent neural networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#links">Links</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-binding-with-sparse-distributed-representations">Variable binding with sparse distributed representations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Links</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-on-functions-using-randomized-vector-representations">Computing on Functions Using Randomized Vector Representations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Links</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#integer-echo-state-networks-efficient-reservoir-computing-for-digital-hardware">Integer Echo State Networks: Efficient Reservoir Computing for Digital Hardware</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Links</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cellular-automata-can-reduce-memory-requirements-of-collective-state-computing">Cellular Automata Can Reduce Memory Requirements of Collective-State Computing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Links</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-symbolic-architectures-as-a-computing-framework-for-emerging-hardware">Vector Symbolic Architectures as a Computing Framework for Emerging Hardware</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Links</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dr. E. Paxon Frady
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>