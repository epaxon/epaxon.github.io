
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Theoretical Neuroscience &#8212; Neuromorphic Algorithms Research</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sparse binding demo – What’s the dollar of mexico?" href="sparse_binding_demo.html" />
    <link rel="prev" title="Threshold Phasor Associative Memory" href="tpam_demo.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/neural_circuit.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Neuromorphic Algorithms Research</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Neuromorphic Algorithms Research
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About me
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../about_me.html">
   E. Paxon Frady
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Research
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="spiking_networks.html">
   Spiking Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="tpam_demo.html">
     Threshold Phasor Associative Memory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="current reference internal" href="#">
   Theoretical Neuroscience
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="sparse_binding_demo.html">
     Sparse binding demo – What’s the dollar of mexico?
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="resonator_networks.html">
   Resonator Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="resonator_template.html">
     Factorization of shape, color and location
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="res_semi_primes-220216.html">
     Factoring semi-primes with the resonator network
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Tutorials/computational_neuroscience.html">
   Computational Neuroscience
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Tutorials/spiking_basics_brian2.html">
     Rate-coding with Integrate-and-Fire neurons
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Tutorials/connecting_if_neurons.html">
     Connecting IFR theory to connectionism
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Tutorials/hodgkin_huxley_paradigm.html">
     The Hodgkin Huxley Paradigm
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Research/theoretical_neuroscience.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/Research/theoretical_neuroscience.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-theory-of-sequence-indexing-and-working-memory-in-recurrent-neural-networks">
   A theory of sequence indexing and working memory in recurrent neural networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary">
     Summary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#links">
     Links
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variable-binding-with-sparse-distributed-representations">
   Variable binding with sparse distributed representations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Summary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Links
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computing-on-functions-using-randomized-vector-representations">
   Computing on Functions Using Randomized Vector Representations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Summary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     Links
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <!-- Table of contents that is only displayed when printing the page -->
    <div id="jb-print-docs-body" class="onlyprint">
        <h1>Theoretical Neuroscience</h1>
        <!-- Table of contents -->
        <div id="print-main-content" class="row">
            <div class="col-12 col-md-12 pl-md-5 pr-md-5">
            <div id="jb-print-toc">
                
                <div>
                    <h2> Contents </h2>
                </div>
                <nav aria-label="Page">
                    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-theory-of-sequence-indexing-and-working-memory-in-recurrent-neural-networks">
   A theory of sequence indexing and working memory in recurrent neural networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary">
     Summary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#links">
     Links
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variable-binding-with-sparse-distributed-representations">
   Variable binding with sparse distributed representations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Summary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Links
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computing-on-functions-using-randomized-vector-representations">
   Computing on Functions Using Randomized Vector Representations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Summary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     Links
    </a>
   </li>
  </ul>
 </li>
</ul>

                </nav>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="theoretical-neuroscience">
<h1>Theoretical Neuroscience<a class="headerlink" href="#theoretical-neuroscience" title="Permalink to this headline">¶</a></h1>
<div class="section" id="a-theory-of-sequence-indexing-and-working-memory-in-recurrent-neural-networks">
<h2>A theory of sequence indexing and working memory in recurrent neural networks<a class="headerlink" href="#a-theory-of-sequence-indexing-and-working-memory-in-recurrent-neural-networks" title="Permalink to this headline">¶</a></h2>
<!--
<img src="image_memory_buffer.png" alt="Memory buffer"
	width="60%"/>
-->
<p><img alt="Image memory buffer" src="../_images/image_memory_buffer.png" /></p>
<div class="section" id="summary">
<h3>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h3>
<p>Randomized recurrent neural networks, denoted liquid state machines or echo-state networks, were popularized based on their abilities to perform useful computations without the constraint of needing stable states. This inpsired many to think of new ways in which neural networks could be used for computation. However, many treated these networks as a black box, and little was understood as to how they performed their computations.</p>
<p>We identified how such randomized networks could be described based on the principles of Vector Symbolic Architectures. We realized that you could describe the operations of an echo-state network as forming a time-tag, attaching the input to the time-tag, and holding the input history in working memory. This allowed us to derive a precise theory of memory performance and scaling, as well as methods to optimize performance and memory of such netowrks.</p>
</div>
<div class="section" id="links">
<h3>Links<a class="headerlink" href="#links" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://direct.mit.edu/neco/article-abstract/30/6/1449/8400/A-Theory-of-Sequence-Indexing-and-Working-Memory">NeCo</a></p>
<p><a class="reference external" href="https://redwood.berkeley.edu/wp-content/uploads/2020/08/frady-sequence-memory.pdf">PDF</a></p>
</div>
</div>
<div class="section" id="variable-binding-with-sparse-distributed-representations">
<h2>Variable binding with sparse distributed representations<a class="headerlink" href="#variable-binding-with-sparse-distributed-representations" title="Permalink to this headline">¶</a></h2>
<!--
<img src="sparse_binding.png" alt="Sparse binding"
	width="75%"/>
-->
<p><img alt="Sparse binding" src="../_images/sparse_binding.png" /></p>
<div class="section" id="id1">
<h3>Summary<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>The idea of binding has been part of the connectionist literature for quite some time. The concept of binding is not addressed in modern deep learning much at all, but often studies attempt to perform binding in unnatural ways.</p>
<p>Connectionists in the early 90s realized that there was a need for a way to bind information together in neural representations, and the “superposition catastrophe” was an often cited problem. Early proposals solved this problem by adding the tensor-product operation to connectionism, and Smolensky developed his theory of tensor-product representations.</p>
<p>The tensor product was key to solving the superposition catastrophe because it was a combinatoric operation, and fundamentally this is the issue. Connectionism needed a way to represent “blue triangle” and “red square” in ways that were unique and non-interfering. This fundamentally leads to needing an operation that has a unique result for every combination of shapes and colors.</p>
<p>The combinatorics of tensor products, however, leads to the undersirable result that the neural representation of information is also growing combinatorically. Hinton and other called for proposals where combinatoric representations could be described, but without the growth in dimensionality of the representation. This is where VSA was born, with Plate’s proposals of how to bind information together in a fixed dimensionality that still has access to combinatoric representations.</p>
<p>Since Plate’s proposals, several VSA models were developed which utilize different representations and binding operations. In this paper, we show how all of these VSA models can be unified with tensor-products through the mathematics of compressed sensing. Basically, all VSA binding operations are reductions of the tensor-product.</p>
<p>One of the biggest issues with linking VSA representations to neuroscience is that VSA models are typically dense representation where every vector element would correspond to neural activation. But representations in the brain are highly sparse. This lead us to explore potential new avenues for binding operations, where we desired a binding operation that maintains sparsity.</p>
<p>Based on the link between binding and tensor-product reductions, we explored several methods that could result in a sparsity- and dimensionality-preserving binding operation. We identified a previous proposal based on sparse block-codes that achieved all the desirable features of a sparse VSA, and illustrate how to perform computations with the VSA.</p>
</div>
<div class="section" id="id2">
<h3>Links<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://arxiv.org/abs/2009.06734">arXiv</a></p>
<p><a class="reference internal" href="sparse_binding_demo.html"><span class="doc std std-doc">Demo</span></a> of sparse binding based on Kanerva’s example <a class="reference external" href="https://redwood.berkeley.edu/wp-content/uploads/2020/05/kanerva2010what.pdf">What’s the dollar of Mexico?</a></p>
</div>
</div>
<div class="section" id="computing-on-functions-using-randomized-vector-representations">
<h2>Computing on Functions Using Randomized Vector Representations<a class="headerlink" href="#computing-on-functions-using-randomized-vector-representations" title="Permalink to this headline">¶</a></h2>
<p><img alt="vfa_kernels" src="../_images/kernels-210322.png" /></p>
<div class="section" id="id3">
<h3>Summary<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>Vector symbolic architectures (VSA) are frameworks for computing with distributed representations like neural networks. There are multiple flavors of the framework, but each is designed to form and manipulate symbols in high-dimensional vector spaces. This is possible, and goes beyond standard artificial neural networks, because of the binding operation in VSA.</p>
<p>In this paper, we extend VSA ideas to vector function architectures (VFA), which can encode and manipulate data on smooth manifolds. This is possible by a fromulation for vector representations of continuous values with fractional power encoding. The idea starts with self-binding of a base vector to create a representation of integers: <span class="math notranslate nohighlight">\(\mathbf{z}^i\)</span> is the vector for representing integer <span class="math notranslate nohighlight">\(i\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{C}^N\)</span> is a random vector. This self-binding process can be extended to continuous values <span class="math notranslate nohighlight">\(x\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{z}^{\beta x}\)</span> represents a continuous number. Such a procedure is a locality-perserving encoding and produces a similarity kernel, where representations of values near <span class="math notranslate nohighlight">\(x\)</span> are similar to the representation for <span class="math notranslate nohighlight">\(x\)</span>, but representations far away are different. The parameter <span class="math notranslate nohighlight">\(\beta\)</span> regulates the width of the similarity kernel.</p>
<p>In the paper, we explain how the VFA can be generalized to other flavors of VSA, such as sparse block-codes. Further, we explain how to create different types of similarity kernels based on the Bochner theorem. Further, we show how multi-dimensional kernels can be formed using the binding operation of 1-D kernels or using jointly sampled base vectors.</p>
<p>Finally, we show a few simple applications that links VFA to other methods in machine-learning, such as kernel methods. The VFA archite</p>
</div>
<div class="section" id="id4">
<h3>Links<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://arxiv.org/abs/2109.03429">arXiv</a></p>
</div>
</div>
<div class="toctree-wrapper compound">
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Research"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </div>
        </div>
    </div>
    <div id="main-content" class="row noprint">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="theoretical-neuroscience">
<h1>Theoretical Neuroscience<a class="headerlink" href="#theoretical-neuroscience" title="Permalink to this headline">¶</a></h1>
<div class="section" id="a-theory-of-sequence-indexing-and-working-memory-in-recurrent-neural-networks">
<h2>A theory of sequence indexing and working memory in recurrent neural networks<a class="headerlink" href="#a-theory-of-sequence-indexing-and-working-memory-in-recurrent-neural-networks" title="Permalink to this headline">¶</a></h2>
<!--
<img src="image_memory_buffer.png" alt="Memory buffer"
	width="60%"/>
-->
<p><img alt="Image memory buffer" src="../_images/image_memory_buffer.png" /></p>
<div class="section" id="summary">
<h3>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h3>
<p>Randomized recurrent neural networks, denoted liquid state machines or echo-state networks, were popularized based on their abilities to perform useful computations without the constraint of needing stable states. This inpsired many to think of new ways in which neural networks could be used for computation. However, many treated these networks as a black box, and little was understood as to how they performed their computations.</p>
<p>We identified how such randomized networks could be described based on the principles of Vector Symbolic Architectures. We realized that you could describe the operations of an echo-state network as forming a time-tag, attaching the input to the time-tag, and holding the input history in working memory. This allowed us to derive a precise theory of memory performance and scaling, as well as methods to optimize performance and memory of such netowrks.</p>
</div>
<div class="section" id="links">
<h3>Links<a class="headerlink" href="#links" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://direct.mit.edu/neco/article-abstract/30/6/1449/8400/A-Theory-of-Sequence-Indexing-and-Working-Memory">NeCo</a></p>
<p><a class="reference external" href="https://redwood.berkeley.edu/wp-content/uploads/2020/08/frady-sequence-memory.pdf">PDF</a></p>
</div>
</div>
<div class="section" id="variable-binding-with-sparse-distributed-representations">
<h2>Variable binding with sparse distributed representations<a class="headerlink" href="#variable-binding-with-sparse-distributed-representations" title="Permalink to this headline">¶</a></h2>
<!--
<img src="sparse_binding.png" alt="Sparse binding"
	width="75%"/>
-->
<p><img alt="Sparse binding" src="../_images/sparse_binding.png" /></p>
<div class="section" id="id1">
<h3>Summary<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>The idea of binding has been part of the connectionist literature for quite some time. The concept of binding is not addressed in modern deep learning much at all, but often studies attempt to perform binding in unnatural ways.</p>
<p>Connectionists in the early 90s realized that there was a need for a way to bind information together in neural representations, and the “superposition catastrophe” was an often cited problem. Early proposals solved this problem by adding the tensor-product operation to connectionism, and Smolensky developed his theory of tensor-product representations.</p>
<p>The tensor product was key to solving the superposition catastrophe because it was a combinatoric operation, and fundamentally this is the issue. Connectionism needed a way to represent “blue triangle” and “red square” in ways that were unique and non-interfering. This fundamentally leads to needing an operation that has a unique result for every combination of shapes and colors.</p>
<p>The combinatorics of tensor products, however, leads to the undersirable result that the neural representation of information is also growing combinatorically. Hinton and other called for proposals where combinatoric representations could be described, but without the growth in dimensionality of the representation. This is where VSA was born, with Plate’s proposals of how to bind information together in a fixed dimensionality that still has access to combinatoric representations.</p>
<p>Since Plate’s proposals, several VSA models were developed which utilize different representations and binding operations. In this paper, we show how all of these VSA models can be unified with tensor-products through the mathematics of compressed sensing. Basically, all VSA binding operations are reductions of the tensor-product.</p>
<p>One of the biggest issues with linking VSA representations to neuroscience is that VSA models are typically dense representation where every vector element would correspond to neural activation. But representations in the brain are highly sparse. This lead us to explore potential new avenues for binding operations, where we desired a binding operation that maintains sparsity.</p>
<p>Based on the link between binding and tensor-product reductions, we explored several methods that could result in a sparsity- and dimensionality-preserving binding operation. We identified a previous proposal based on sparse block-codes that achieved all the desirable features of a sparse VSA, and illustrate how to perform computations with the VSA.</p>
</div>
<div class="section" id="id2">
<h3>Links<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://arxiv.org/abs/2009.06734">arXiv</a></p>
<p><a class="reference internal" href="sparse_binding_demo.html"><span class="doc std std-doc">Demo</span></a> of sparse binding based on Kanerva’s example <a class="reference external" href="https://redwood.berkeley.edu/wp-content/uploads/2020/05/kanerva2010what.pdf">What’s the dollar of Mexico?</a></p>
</div>
</div>
<div class="section" id="computing-on-functions-using-randomized-vector-representations">
<h2>Computing on Functions Using Randomized Vector Representations<a class="headerlink" href="#computing-on-functions-using-randomized-vector-representations" title="Permalink to this headline">¶</a></h2>
<p><img alt="vfa_kernels" src="../_images/kernels-210322.png" /></p>
<div class="section" id="id3">
<h3>Summary<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>Vector symbolic architectures (VSA) are frameworks for computing with distributed representations like neural networks. There are multiple flavors of the framework, but each is designed to form and manipulate symbols in high-dimensional vector spaces. This is possible, and goes beyond standard artificial neural networks, because of the binding operation in VSA.</p>
<p>In this paper, we extend VSA ideas to vector function architectures (VFA), which can encode and manipulate data on smooth manifolds. This is possible by a fromulation for vector representations of continuous values with fractional power encoding. The idea starts with self-binding of a base vector to create a representation of integers: <span class="math notranslate nohighlight">\(\mathbf{z}^i\)</span> is the vector for representing integer <span class="math notranslate nohighlight">\(i\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{C}^N\)</span> is a random vector. This self-binding process can be extended to continuous values <span class="math notranslate nohighlight">\(x\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{z}^{\beta x}\)</span> represents a continuous number. Such a procedure is a locality-perserving encoding and produces a similarity kernel, where representations of values near <span class="math notranslate nohighlight">\(x\)</span> are similar to the representation for <span class="math notranslate nohighlight">\(x\)</span>, but representations far away are different. The parameter <span class="math notranslate nohighlight">\(\beta\)</span> regulates the width of the similarity kernel.</p>
<p>In the paper, we explain how the VFA can be generalized to other flavors of VSA, such as sparse block-codes. Further, we explain how to create different types of similarity kernels based on the Bochner theorem. Further, we show how multi-dimensional kernels can be formed using the binding operation of 1-D kernels or using jointly sampled base vectors.</p>
<p>Finally, we show a few simple applications that links VFA to other methods in machine-learning, such as kernel methods. The VFA archite</p>
</div>
<div class="section" id="id4">
<h3>Links<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://arxiv.org/abs/2109.03429">arXiv</a></p>
</div>
</div>
<div class="toctree-wrapper compound">
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Research"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="tpam_demo.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Threshold Phasor Associative Memory</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="sparse_binding_demo.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Sparse binding demo – What’s the dollar of mexico?</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Dr. E. Paxon Frady<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>