{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e22e089",
   "metadata": {},
   "source": [
    "# Theoretical Neuroscience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0a9a0",
   "metadata": {},
   "source": [
    "## A theory of sequence indexing and working memory in recurrent neural networks\n",
    "\n",
    "<!--\n",
    "<img src=\"image_memory_buffer.png\" alt=\"Memory buffer\"\n",
    "\twidth=\"60%\"/>\n",
    "-->\n",
    "\n",
    "![Image memory buffer](image_memory_buffer.png)\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "Randomized recurrent neural networks, denoted liquid state machines or echo-state networks, were popularized based on their abilities to perform useful computations without the constraint of needing stable states. This inpsired many to think of new ways in which neural networks could be used for computation. However, many treated these networks as a black box, and little was understood as to how they performed their computations.\n",
    "\n",
    "We identified how such randomized networks could be described based on the principles of Vector Symbolic Architectures. We realized that you could describe the operations of an echo-state network as forming a time-tag, attaching the input to the time-tag, and holding the input history in working memory. This allowed us to derive a precise theory of memory performance and scaling, as well as methods to optimize performance and memory of such netowrks. \n",
    "\n",
    "\n",
    "### Links\n",
    "\n",
    "[NeCo](https://direct.mit.edu/neco/article-abstract/30/6/1449/8400/A-Theory-of-Sequence-Indexing-and-Working-Memory)\n",
    "\n",
    "[PDF](https://redwood.berkeley.edu/wp-content/uploads/2020/08/frady-sequence-memory.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a52e1b",
   "metadata": {},
   "source": [
    "## Variable binding with sparse distributed representations\n",
    "\n",
    "<!--\n",
    "<img src=\"sparse_binding.png\" alt=\"Sparse binding\"\n",
    "\twidth=\"75%\"/>\n",
    "-->\n",
    "\n",
    "![Sparse binding](sparse_binding.png)\n",
    "\n",
    "### Summary\n",
    "\n",
    "The idea of binding has been part of the connectionist literature for quite some time. The concept of binding is not addressed in modern deep learning much at all, but often studies attempt to perform binding in unnatural ways.\n",
    "\n",
    "Connectionists in the early 90s realized that there was a need for a way to bind information together in neural representations, and the \"superposition catastrophe\" was an often cited problem. Early proposals solved this problem by adding the tensor-product operation to connectionism, and Smolensky developed his theory of tensor-product representations. \n",
    "\n",
    "The tensor product was key to solving the superposition catastrophe because it was a combinatoric operation, and fundamentally this is the issue. Connectionism needed a way to represent \"blue triangle\" and \"red square\" in ways that were unique and non-interfering. This fundamentally leads to needing an operation that has a unique result for every combination of shapes and colors.\n",
    "\n",
    "The combinatorics of tensor products, however, leads to the undersirable result that the neural representation of information is also growing combinatorically. Hinton and other called for proposals where combinatoric representations could be described, but without the growth in dimensionality of the representation. This is where VSA was born, with Plate's proposals of how to bind information together in a fixed dimensionality that still has access to combinatoric representations. \n",
    "\n",
    "Since Plate's proposals, several VSA models were developed which utilize different representations and binding operations. In this paper, we show how all of these VSA models can be unified with tensor-products through the mathematics of compressed sensing. Basically, all VSA binding operations are reductions of the tensor-product.\n",
    "\n",
    "One of the biggest issues with linking VSA representations to neuroscience is that VSA models are typically dense representation where every vector element would correspond to neural activation. But representations in the brain are highly sparse. This lead us to explore potential new avenues for binding operations, where we desired a binding operation that maintains sparsity. \n",
    "\n",
    "Based on the link between binding and tensor-product reductions, we explored several methods that could result in a sparsity- and dimensionality-preserving binding operation. We identified a previous proposal based on sparse block-codes that achieved all the desirable features of a sparse VSA, and illustrate how to perform computations with the VSA. \n",
    "\n",
    "### Links\n",
    "\n",
    "[arXiv](https://arxiv.org/abs/2009.06734)\n",
    "\n",
    "\n",
    "[Demo](sparse_binding_demo.ipynb) of sparse binding based on Kanerva's example [What's the dollar of Mexico?](https://redwood.berkeley.edu/wp-content/uploads/2020/05/kanerva2010what.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c495b",
   "metadata": {},
   "source": [
    "## Computing on Functions Using Randomized Vector Representations\n",
    "\n",
    "![vfa_kernels](kernels-210322.png)\n",
    "\n",
    "### Summary\n",
    "\n",
    "Vector symbolic architectures (VSA) are frameworks for computing with distributed representations like neural networks. There are multiple flavors of the framework, but each is designed to form and manipulate symbols in high-dimensional vector spaces. This is possible, and goes beyond standard artificial neural networks, because of the binding operation in VSA. \n",
    "\n",
    "In this paper, we extend VSA ideas to vector function architectures (VFA), which can encode and manipulate data on smooth manifolds. This is possible by a fromulation for vector representations of continuous values with fractional power encoding. The idea starts with self-binding of a base vector to create a representation of integers: $\\mathbf{z}^i$ is the vector for representing integer $i$, where $\\mathbf{z} \\in \\mathbb{C}^N$ is a random vector. This self-binding process can be extended to continuous values $x$, where $\\mathbf{z}^{\\beta x}$ represents a continuous number. Such a procedure is a locality-perserving encoding and produces a similarity kernel, where representations of values near $x$ are similar to the representation for $x$, but representations far away are different. The parameter $\\beta$ regulates the width of the similarity kernel. \n",
    "\n",
    "In the paper, we explain how the VFA can be generalized to other flavors of VSA, such as sparse block-codes. Further, we explain how to create different types of similarity kernels based on the Bochner theorem. Further, we show how multi-dimensional kernels can be formed using the binding operation of 1-D kernels or using jointly sampled base vectors. \n",
    "\n",
    "Finally, we show a few simple applications that links VFA to other methods in machine-learning, such as kernel methods. The VFA archite\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Links\n",
    "\n",
    "[arXiv](https://arxiv.org/abs/2109.03429)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e2052b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
